{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2818c8e",
   "metadata": {},
   "source": [
    "# Alzheimer's Disease Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ebc754",
   "metadata": {},
   "source": [
    "### This code will perform stratified five fold cross validation at the subject level using a VGG16 binary classifier. We will also employ a nested-four-fold cross validaion to optimize the epochs and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954a909-0372-41d8-a0c3-566372236f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to start with \n",
    "\n",
    "import os \n",
    "import imageio \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import numpy as np \n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc85605",
   "metadata": {},
   "source": [
    "# 1. Data Splitting and Resizing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22eb30",
   "metadata": {},
   "source": [
    "## 1.1 Initialize a seed for the random data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ad7a0-062a-481e-8a1f-97e01cf9b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_number = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afb4c6",
   "metadata": {},
   "source": [
    "## 1.2  Designate \"1\" for AD, and \"0\" for NC. We will run the data split for each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a842e66",
   "metadata": {},
   "source": [
    "Split data according to their id and distribution of one or two eyes for evenness.\n",
    "- 6180 refers to 61AD, 80NC subjects\n",
    "- The data split here is hard-coded specifically with respect to 100 / 5 = 20 images, hence 20 images per fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2394ebb-996a-463e-bb75-ebbc04e3cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/DATA/charlie/fundus/AutoMorph_6180/AD/binary_vessel/resize/'\n",
    "outer_dir = '/DATA/charlie/fundus/AutoMorph_6180/Experiment/'\n",
    "flag = '1' # 1 here for AD\n",
    "\n",
    "name_of_experiment = 'AutoMorph_6180_' + str(seed_number)\n",
    "images = os.listdir(img_dir)\n",
    "\n",
    "\n",
    "# the subject_ids here are based on the naming of the image\n",
    "# e.g., 1234567_21015_0_0 where 1234567 is the subject ID, 21015_0_0 means left eye at first visit.\n",
    "# the indices (e.g., 0:7) need to be the indicies for the subject id\n",
    "subject_ids = [] \n",
    "for img in images:\n",
    "    eid = img[0:7]\n",
    "    subject_ids.append(np.int(eid))\n",
    "    \n",
    "single_img_id = []\n",
    "double_img_id = [] \n",
    "\n",
    "unique, counts = np.unique(subject_ids, return_counts = True)\n",
    "\n",
    "result = np.column_stack((unique, counts))\n",
    "\n",
    "\n",
    "for i in result:\n",
    "    if i[1] == 2:\n",
    "        double_img_id.append(i[0]) # The i[0] is because of the double brackets\n",
    "    if i[1] == 1:\n",
    "        single_img_id.append(i[0])\n",
    "        \n",
    "random.seed(seed_number)\n",
    "random.shuffle(double_img_id)\n",
    "random.shuffle(single_img_id)\n",
    "\n",
    "if not os.path.exists(outer_dir + name_of_experiment):\n",
    "\n",
    "    os.makedirs(outer_dir + name_of_experiment)\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f1/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f1/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f2/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f2/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f2/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f3')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f3/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f3/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f4/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f4/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f4/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f5/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f5/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f5/0/')\n",
    "else:\n",
    "    print('Folder already exists')\n",
    "    \n",
    "# The flag means whether you are saving CN images or IAD images\n",
    "\n",
    "\n",
    "dir_1 = outer_dir + name_of_experiment + '/f1/' + flag + '/'\n",
    "dir_2 = outer_dir + name_of_experiment + '/f2/' + flag + '/'\n",
    "dir_3 = outer_dir + name_of_experiment + '/f3/' + flag + '/'\n",
    "dir_4 = outer_dir + name_of_experiment + '/f4/' + flag + '/'\n",
    "dir_5 = outer_dir + name_of_experiment + '/f5/' + flag + '/'\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "for eid in double_img_id:    \n",
    "    for img in images:\n",
    "        if str(eid) in img:\n",
    "            X = Image.open(img_dir + img)\n",
    "            X = X.resize((224,224))\n",
    "            if counter % 5 == 0:\n",
    "                X.save(dir_1 + img)\n",
    "            #    print('Image has been saved:' + dir_1 + img)\n",
    "            if counter % 5 == 1:\n",
    "                X.save(dir_2 + img)\n",
    "            #    print('Image has been saved:' +  dir_2 + img)\n",
    "            if counter % 5 == 2:\n",
    "                X.save(dir_3 + img)\n",
    "            #    print('Image has been saved:' +  dir_3 + img)\n",
    "            if counter % 5 == 3:\n",
    "                X.save(dir_4 + img)  \n",
    "            #    print('Image has been saved:' + dir_4 + img)\n",
    "            if counter % 5 == 4:\n",
    "                X.save(dir_5 + img)  \n",
    "             #   print('Image has been saved:' + dir_4 + img)\n",
    "    #print('---------------------------')\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "#print('Now starting to print out the single images')\n",
    "for eid in single_img_id:\n",
    "    for img in images:\n",
    "        if str(eid) in img:\n",
    "            X = Image.open(img_dir + img)\n",
    "            X = X.resize((224,224))\n",
    "            if len(os.listdir(dir_1)) < 20:\n",
    "                X.save(dir_1 + img)\n",
    "           #     print('Image has been saved:' + dir_1 + img)\n",
    "            elif len(os.listdir(dir_2)) < 20:\n",
    "                X.save(dir_2 + img)\n",
    "            #    print('Image has been saved:' + dir_2 + img)\n",
    "            elif len(os.listdir(dir_3)) < 20:\n",
    "                X.save(dir_3 + img)\n",
    "            #    print('Image has been saved:' + dir_3 + img)\n",
    "            elif len(os.listdir(dir_4)) < 20:\n",
    "                X.save(dir_4 + img)\n",
    "            #    print('Image has been saved:' + dir_4 + img)\n",
    "            elif len(os.listdir(dir_5)) < 20:\n",
    "                X.save(dir_5 + img)\n",
    "            #    print('Image has been saved:' + dir_5 + img)\n",
    "print('Data has been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee9d48-28e4-4df5-bea9-ac14c1fbd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/DATA/charlie/fundus/AutoMorph_6180/CN/binary_vessel/resize/'\n",
    "outer_dir = '/DATA/charlie/fundus/AutoMorph_6180/Experiment/'\n",
    "flag = '0' # Your options here at '1' and '0'\n",
    "\n",
    "name_of_experiment = 'AutoMorph_6180_' + str(seed_number)\n",
    "\n",
    "\n",
    "images = os.listdir(img_dir)\n",
    "\n",
    "subject_ids = [] \n",
    "for img in images:\n",
    "    #eid = img[3:10]\n",
    "    eid = img[3:10]\n",
    "    # if AD then 0:7\n",
    "    # if CN then 3:10\n",
    "    subject_ids.append(np.int(eid))\n",
    "    \n",
    "single_img_id = []\n",
    "double_img_id = [] \n",
    "\n",
    "unique, counts = np.unique(subject_ids, return_counts = True)\n",
    "\n",
    "result = np.column_stack((unique, counts))\n",
    "\n",
    "\n",
    "for i in result:\n",
    "    if i[1] == 2:\n",
    "        double_img_id.append(i[0]) # The i[0] is because of the double brackets\n",
    "    if i[1] == 1:\n",
    "        single_img_id.append(i[0])\n",
    "        \n",
    "random.seed(seed_number)\n",
    "random.shuffle(double_img_id)\n",
    "random.shuffle(single_img_id)\n",
    "\n",
    "if not os.path.exists(outer_dir + name_of_experiment):\n",
    "\n",
    "    os.makedirs(outer_dir + name_of_experiment)\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f1/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f1/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f2/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f2/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f2/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f3')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f3/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f3/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f4/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f4/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f4/0/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f5/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f5/1/')\n",
    "    os.makedirs(outer_dir + name_of_experiment + '/f5/0/')\n",
    "else:\n",
    "    print('Folder already exists')\n",
    "    \n",
    "# The flag means whether you are saving CN images or IAD images\n",
    "\n",
    "\n",
    "dir_1 = outer_dir + name_of_experiment + '/f1/' + flag + '/'\n",
    "dir_2 = outer_dir + name_of_experiment + '/f2/' + flag + '/'\n",
    "dir_3 = outer_dir + name_of_experiment + '/f3/' + flag + '/'\n",
    "dir_4 = outer_dir + name_of_experiment + '/f4/' + flag + '/'\n",
    "dir_5 = outer_dir + name_of_experiment + '/f5/' + flag + '/'\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "for eid in double_img_id:    \n",
    "    for img in images:\n",
    "        if str(eid) in img:\n",
    "            X = Image.open(img_dir + img)\n",
    "            X = X.resize((224,224))\n",
    "            if counter % 5 == 0:\n",
    "                X.save(dir_1 + img)\n",
    "             #   print('Image has been saved:' + dir_1 + img)\n",
    "            if counter % 5 == 1:\n",
    "                X.save(dir_2 + img)\n",
    "             #   print('Image has been saved:' +  dir_2 + img)\n",
    "            if counter % 5 == 2:\n",
    "                X.save(dir_3 + img)\n",
    "             #   print('Image has been saved:' +  dir_3 + img)\n",
    "            if counter % 5 == 3:\n",
    "                X.save(dir_4 + img)  \n",
    "             #   print('Image has been saved:' + dir_4 + img)\n",
    "            if counter % 5 == 4:\n",
    "                X.save(dir_5 + img)  \n",
    "             #   print('Image has been saved:' + dir_5 + img)\n",
    "    #print('---------------------------')\n",
    "    counter += 1\n",
    "\n",
    "#print('Data has been saved')\n",
    "#print('Now starting to print out the single images')\n",
    "for eid in single_img_id:\n",
    "    for img in images:\n",
    "        if str(eid) in img:\n",
    "            X = Image.open(img_dir + img)\n",
    "            X = X.resize((224,224))\n",
    "            if len(os.listdir(dir_1)) < 20:\n",
    "                X.save(dir_1 + img)\n",
    "             #   print('Image has been saved:' + dir_1 + img)\n",
    "            elif len(os.listdir(dir_2)) < 20:\n",
    "                X.save(dir_2 + img)\n",
    "             #   print('Image has been saved:' + dir_2 + img)\n",
    "            elif len(os.listdir(dir_3)) < 20:\n",
    "                X.save(dir_3 + img)\n",
    "             #   print('Image has been saved:' + dir_3 + img)\n",
    "            elif len(os.listdir(dir_4)) < 20:\n",
    "                X.save(dir_4 + img)\n",
    "             #   print('Image has been saved:' + dir_4 + img)\n",
    "            elif len(os.listdir(dir_5)) < 20:\n",
    "                X.save(dir_5 + img)\n",
    "             #   print('Image has been saved:' + dir_5 + img)\n",
    "print('Data has been saved')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9e151",
   "metadata": {},
   "source": [
    "# 2. Model Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a08fc2",
   "metadata": {},
   "source": [
    "# 2.1 Packages\n",
    "\n",
    "The traditional PyTorch (compatibility tested at 1.7.1) and sklearn packages are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3639a97-3ca7-42e6-8973-c72f8964e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f12a6",
   "metadata": {},
   "source": [
    "# 2.2 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac739ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAUC(dataGT, dataPRED):\n",
    "    outAUROC = []\n",
    "    datanpGT = dataGT.cpu().numpy()\n",
    "    datanpPRED = dataPRED.cpu().numpy()\n",
    "            \n",
    "    return roc_auc_score(datanpGT, datanpPRED)\n",
    "\n",
    "def computeACC(dataGT, dataCLASS):\n",
    "    \n",
    "    datanpGT = dataGT.cpu().numpy()\n",
    "    datanpCLASS = dataCLASS.cpu().numpy()\n",
    "    \n",
    "    return accuracy_score(datanpGT, datanpCLASS)\n",
    "\n",
    "def classreport(dataGT, predCLASS):\n",
    "    datanpGT = dataGT.cpu().numpy()\n",
    "    datanppredCLASS = predCLASS.cpu().numpy()\n",
    "    \n",
    "    print(classification_report(datanpGT, datanppredCLASS, digits = 3))\n",
    "    \n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train() \n",
    "    tr_loss = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = data\n",
    "        images = images.float().cuda()\n",
    "        labels = labels.long().cuda()\n",
    "    #    print(images.shape)\n",
    "        output = model(images)\n",
    "        softmax_preds = F.softmax(output, dim = 1)\n",
    "        loss = criterion(softmax_preds, labels)\n",
    "      #  print('Loss is', loss.item())\n",
    "        tr_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.average(tr_loss, axis = 0)\n",
    "\n",
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    outGT = torch.FloatTensor().cuda()\n",
    "    outPRED = torch.FloatTensor().cuda()\n",
    "    outClass = torch.FloatTensor().cuda()\n",
    "    val_loss = [] \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            images, labels, = data\n",
    "            images = images.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "            output = model(images)\n",
    "            softmax_preds = F.softmax(output, dim = 1)\n",
    "            loss = criterion(softmax_preds, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            outGT = torch.cat((outGT, labels), 0)\n",
    "            outPRED = torch.cat((outPRED, softmax_preds), 0)\n",
    "            class_predictions = torch.argmax(softmax_preds, dim = 1)\n",
    "            outClass = torch.cat((outClass, class_predictions), 0)\n",
    "    acc_test = computeACC(outGT, outClass)\n",
    "    return np.average(val_loss, axis= 0 ), acc_test\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    outGT = torch.FloatTensor().cuda()\n",
    "    outPRED = torch.FloatTensor().cuda()\n",
    "    outClass = torch.FloatTensor().cuda()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            images, labels, = data\n",
    "            images = images.float().cuda()\n",
    "            labels = labels.float().cuda()\n",
    "            output = model(images)\n",
    "            outGT = torch.cat((outGT, labels), 0)\n",
    "            softmax_preds = F.softmax(output, dim = 1)\n",
    "            outPRED = torch.cat((outPRED, softmax_preds), 0)\n",
    "            class_predictions = torch.argmax(softmax_preds, dim = 1)\n",
    "            outClass = torch.cat((outClass, class_predictions), 0)\n",
    "        \n",
    "    classreport(outGT, outClass)\n",
    "    acc_test = computeACC(outGT, outClass)\n",
    "    return acc_test\n",
    "\n",
    "def nested_data_order(data, tr_transform, val_transform, N):\n",
    "    TEST_DATA = datasets.ImageFolder(root = DATA_SETS[4])\n",
    "    if N == 1:\n",
    "        TRAIN_DATA_F1 = datasets.ImageFolder(root = DATA_SETS[0], transform = tr_transform)\n",
    "        TRAIN_DATA_F2 = datasets.ImageFolder(root = DATA_SETS[1], transform = tr_transform)\n",
    "        TRAIN_DATA_F3 = datasets.ImageFolder(root = DATA_SETS[2], transform = tr_transform)\n",
    "        TRAIN_DATA = ConcatDataset([TRAIN_DATA_F1, TRAIN_DATA_F2, TRAIN_DATA_F3])\n",
    "        VAL_DATA = datasets.ImageFolder(root = DATA_SETS[3], transform = val_transform)\n",
    "        \n",
    "\n",
    "    if N == 2:\n",
    "        TRAIN_DATA_F1 = datasets.ImageFolder(root = DATA_SETS[0], transform = tr_transform)\n",
    "        TRAIN_DATA_F2 = datasets.ImageFolder(root = DATA_SETS[1], transform = tr_transform)\n",
    "        TRAIN_DATA_F3 = datasets.ImageFolder(root = DATA_SETS[3], transform = tr_transform)\n",
    "        TRAIN_DATA = ConcatDataset([TRAIN_DATA_F1, TRAIN_DATA_F2, TRAIN_DATA_F3])\n",
    "        VAL_DATA = datasets.ImageFolder(root = DATA_SETS[2], transform = val_transform)\n",
    "    \n",
    "        \n",
    "    if N == 3:\n",
    "        TRAIN_DATA_F1 = datasets.ImageFolder(root = DATA_SETS[0], transform = tr_transform)\n",
    "        TRAIN_DATA_F2 = datasets.ImageFolder(root = DATA_SETS[2], transform = tr_transform)\n",
    "        TRAIN_DATA_F3 = datasets.ImageFolder(root = DATA_SETS[3], transform = tr_transform)\n",
    "        TRAIN_DATA = ConcatDataset([TRAIN_DATA_F1, TRAIN_DATA_F2, TRAIN_DATA_F3])\n",
    "        VAL_DATA = datasets.ImageFolder(root = DATA_SETS[1], transform = val_transform)\n",
    "        \n",
    "    if N == 4:\n",
    "        TRAIN_DATA_F1 = datasets.ImageFolder(root = DATA_SETS[1], transform = tr_transform)\n",
    "        TRAIN_DATA_F2 = datasets.ImageFolder(root = DATA_SETS[2], transform = tr_transform)\n",
    "        TRAIN_DATA_F3 = datasets.ImageFolder(root = DATA_SETS[3], transform = tr_transform)\n",
    "        TRAIN_DATA = ConcatDataset([TRAIN_DATA_F1, TRAIN_DATA_F2, TRAIN_DATA_F3])\n",
    "        VAL_DATA = datasets.ImageFolder(root = DATA_SETS[0], transform = val_transform)\n",
    "\n",
    "\n",
    "    return TRAIN_DATA, VAL_DATA, TEST_DATA\n",
    "\n",
    "def generate_train_test_set(data, train_transform, test_transform):\n",
    "    \n",
    "    \n",
    "    TRAIN_DATA_F1 = datasets.ImageFolder(root = DATA_SETS[0], transform = train_transform)\n",
    "    TRAIN_DATA_F2 = datasets.ImageFolder(root = DATA_SETS[1], transform = train_transform)\n",
    "    TRAIN_DATA_F3 = datasets.ImageFolder(root = DATA_SETS[2], transform = train_transform)\n",
    "    TRAIN_DATA_F4 = datasets.ImageFolder(root = DATA_SETS[3], transform = train_transform)\n",
    "    TRAIN_DATA = ConcatDataset([TRAIN_DATA_F1, TRAIN_DATA_F2, TRAIN_DATA_F3, TRAIN_DATA_F4])\n",
    "    TEST_DATA = datasets.ImageFolder(root = DATA_SETS[4], transform = test_transform)\n",
    "    \n",
    "    return TRAIN_DATA, TEST_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f9640",
   "metadata": {},
   "source": [
    "# 2.3: 5 Fold Cross Validation\n",
    "\n",
    "We will do a five fold cross validation. There will be a four fold inner cross validation to optimize the learning rate and epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25c8f7-5513-4433-8de2-489b3c2f11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#primary_directory = '/blue/ruogu.fang/charlietran/UKB/code/AutoMorph/Experiment/AutoMorph_6180_' + str(seed_number)\n",
    "primary_directory = outer_dir + name_of_experiment\n",
    "fold1_img_dir = primary_directory + '/f1/'\n",
    "fold2_img_dir = primary_directory + '/f2/'\n",
    "fold3_img_dir = primary_directory + '/f3/'\n",
    "fold4_img_dir = primary_directory + '/f4/'\n",
    "fold5_img_dir = primary_directory + '/f5/'\n",
    "\n",
    "\n",
    "    \n",
    "DATA = [[fold1_img_dir, fold2_img_dir, fold3_img_dir, fold4_img_dir, fold5_img_dir],\n",
    "       [fold1_img_dir, fold2_img_dir, fold3_img_dir, fold5_img_dir, fold4_img_dir],\n",
    "       [fold1_img_dir, fold2_img_dir, fold4_img_dir, fold5_img_dir, fold3_img_dir],\n",
    "       [fold1_img_dir, fold3_img_dir, fold4_img_dir, fold5_img_dir, fold2_img_dir],\n",
    "       [fold2_img_dir, fold3_img_dir, fold4_img_dir, fold5_img_dir, fold1_img_dir]]\n",
    "\n",
    "for j in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    print('STARTING PROCEDURE FOR TEST FOLD' + str(j + 1))\n",
    "    ###############\n",
    "    num_classes = 2\n",
    "    learning_rate = [1e-4, 1e-5]\n",
    "    DATA_SETS = DATA[j]\n",
    "    #DATA_SETS = [fold1_img_dir, fold2_img_dir, fold3_img_dir, fold4_img_dir, fold5_img_dir]\n",
    "    N = 4\n",
    "    num_epochs = 50\n",
    "    bsz = 64\n",
    "\n",
    "\n",
    "    rotations = transforms.RandomRotation(15)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomHorizontalFlip(p =0.5),\n",
    "            transforms.RandomVerticalFlip(p = 0.5),\n",
    "        transforms.RandomApply([rotations], p = 0.5),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimal_accuracy_list = []\n",
    "    optimal_epochs_list = [] \n",
    "    for learn_rate in learning_rate:\n",
    "        inner_accuracy = []\n",
    "        epoch_list = []\n",
    "\n",
    "        inner_accuracy = []\n",
    "        epoch_list = []\n",
    "\n",
    "        for inner_n in range(1, N + 1):\n",
    "            print(f'INNERFOLD {inner_n}', ' LEARNING RATE', learn_rate)\n",
    "            print('--------------------------------')\n",
    "            train_ncv_data, val_cv_data, test_data = nested_data_order(DATA_SETS, train_transform, test_transform, inner_n)\n",
    "            train_nest_loader = DataLoader(train_ncv_data, batch_size = bsz, shuffle = True, drop_last = True)\n",
    "            val_nest_loader = DataLoader(val_cv_data, batch_size = bsz, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "            model = models.vgg16(pretrained = True)\n",
    "            model.classifier[6] = nn.Linear(in_features = 4096, out_features = num_classes, bias = True)\n",
    "            model.cuda()\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss().cuda()\n",
    "            optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n",
    "            #optimizer = optim.Adam(model.parameters(), lr = learn_rate, weight_decay = 1e-6)\n",
    "            print('INITIALIZING THE VGG16 MODEL TO IMAGENET')\n",
    "\n",
    "            epochs_list = [] \n",
    "            validation_loss_set = [] \n",
    "            training_loss_set = []\n",
    "\n",
    "            nest_val_acc = [] \n",
    "            for epoch in range(num_epochs):\n",
    "                training_loss = train(model, train_nest_loader, optimizer, criterion)\n",
    "                validation_loss, val_accuracy = validation(model, val_nest_loader)\n",
    "                #print(\"Epoch\", epoch, 'Training Loss', \"{:.4f}\".format(training_loss), \n",
    "                #      ' Validation Loss', \"{:.4f}\".format(validation_loss), ' Validation Accuracy', val_accuracy)\n",
    "                epochs_list.append(epoch)\n",
    "                validation_loss_set.append(validation_loss)\n",
    "                training_loss_set.append(training_loss)\n",
    "\n",
    "                nest_val_acc.append(val_accuracy)\n",
    "\n",
    "            inner_accuracy.append(nest_val_acc)\n",
    "            # nest val acc is now a list of 50 elements\n",
    "\n",
    "                #print('Validation Accuracy', val_accuracy)\n",
    "                #inner_accuracy.append(val_accuracy)\n",
    "            #plt.plot(epochs_list, training_loss_set)\n",
    "            #plt.title('Training Loss Curve' + ' Test fold ' + 'Five' + ' Internal fold ' + str(inner_n) + ' learning rate' + str(learn_rate))\n",
    "            #plt.plot(epochs_list, validation_loss_set)\n",
    "            #plt.title('Validation Loss Curve' + 'Test fold' + 'Five' + ' Internal fold ' + str(inner_n) + ' learning rate' + str(learn_rate))\n",
    "            #plt.legend(['Training', 'Validation'])\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        optimal_accuracy_list.append(max(np.average(inner_accuracy, axis = 0)))\n",
    "        optimal_epochs_list.append(np.argmax(np.average(inner_accuracy, axis = 0)))\n",
    "\n",
    "    optimal_learning_rate = learning_rate[np.argmax(optimal_accuracy_list, axis = 0)]\n",
    "    optimal_epochs = optimal_epochs_list[np.argmax(optimal_accuracy_list, axis = 0)]\n",
    "    print('Optimal Accuracy List is', optimal_accuracy_list, 'Optimal Epochs list is', optimal_epochs_list)\n",
    "    print('The optimal learning rate is', optimal_learning_rate, ' the optimal epochs is ', optimal_epochs)\n",
    "\n",
    "    train_data, test_data = generate_train_test_set(DATA_SETS, train_transform, test_transform)\n",
    "    train_loader = DataLoader(train_data, batch_size = bsz, shuffle = True, drop_last = True)\n",
    "    test_loader = DataLoader(test_data, batch_size = bsz, shuffle = False) # Shuffle does not really  matter \n",
    "    model = models.vgg16(pretrained = True)\n",
    "    model.classifier[6] = nn.Linear(in_features = 4096, out_features = num_classes, bias = True)\n",
    "    model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr = learn_rate, weight_decay = 1e-6)\n",
    "\n",
    "\n",
    "\n",
    "    epochs = []\n",
    "    epoch_training_loss = []\n",
    "    epoch_val_loss = []\n",
    "    for epoch in range(optimal_epochs):\n",
    "        training_loss = train(model, train_loader, optimizer, criterion)\n",
    "        epoch_training_loss.append(training_loss)\n",
    "        epochs.append(epoch)\n",
    "        print(\"Epoch\", epoch, 'Training Loss', \"{:.4f}\".format(training_loss))\n",
    "\n",
    "    acc_test = test(model, test_loader)   \n",
    "\n",
    "\n",
    "    output_save_model_name = '/home/charlietran/model_auto/VGG16_AutoMorph_TESTFOLD_' + str(j+1) + '_6180_' + str(seed_number) + '.pth'\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "        }, output_save_model_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
